import requests
import random
import time

import torch

SERVER_URL = "http://127.0.0.1:2268"
USE_STREAM = True  # ä¿®æ”¹ä¸º False èµ°éæµå¼ /generate

print(torch.cuda.is_available())

example_prompts = [
    "è¯·è§£é‡Šé‡å­çº ç¼ çš„åŸºæœ¬åŸç†ã€‚",
    "ä½ å¯¹äººå·¥æ™ºèƒ½æœªæ¥å‘å±•çš„çœ‹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ",
    "å†™ä¸€é¦–å…³äºå­¤ç‹¬ä¸åšéŸ§çš„ç°ä»£è¯—ã€‚",
    "ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ Llama æ¨¡å‹çš„ç»“æ„ã€‚",
    "ç»™å‡ºä¸€ä¸ªä½¿ç”¨ Python å®ç°å¿«æ’çš„ä¾‹å­ã€‚",
    "ä»‹ç»ä¸€ä¸‹ Gigabyteã€‚",
    "ä»€ä¹ˆæ˜¯æ–‡è‰ºå¤å…´ï¼Ÿå®ƒå¯¹ä¸–ç•Œäº§ç”Ÿäº†ä»€ä¹ˆå½±å“ï¼Ÿ",
    "è¯·ç®€è¿°åº„å­çš„â€œé€é¥æ¸¸â€æ€æƒ³ã€‚",
    "What are the AI acceleration capabilities of the AMD Ryzen AI Max+ Pro 395?",
    "How does the Apple M4 Max compare to previous Apple Silicon chips in performance and efficiency?",
    "Explain how a CPU works.",
    "How does a GPU accelerate neural networks?",
    "What is virtualization and how does it relate to QEMU?",
    "Describe the architecture of the LLaMA language model.",
]

for i in range(5):
    prompt = random.choice(example_prompts)
    payload = {
        "prompt": prompt,
        "max_tokens": 131072,
        "stream": USE_STREAM
    }

    print(f"\nğŸ“¤ ç¬¬ {i+1} æ¬¡è¯·æ±‚ï¼ŒPrompt: {prompt}")

    try:
        if USE_STREAM:
            with requests.post(f"{SERVER_URL}/stream_generate", json=payload, stream=True) as r:
                r.raise_for_status()
                print("ğŸ“¥ æ¨¡å‹æµå¼è¾“å‡ºï¼š", end="", flush=True)
                for chunk in r.iter_content(chunk_size=None):
                    print(chunk.decode("utf-8"), end="", flush=True)
            print()  # æ¢è¡Œ
        else:
            response = requests.post(f"{SERVER_URL}/generate", json=payload)
            response.raise_for_status()
            result = response.json()
            print("ğŸ“¥ æ¨¡å‹å®Œæ•´å›å¤:", result["result"].strip())

    except Exception as e:
        print("âŒ è¯·æ±‚å¤±è´¥:", e)

    time.sleep(1)
